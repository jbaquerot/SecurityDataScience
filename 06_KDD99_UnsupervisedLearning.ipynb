{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![GMV](https://www.gmv.com/export/system/modules/com.gmv.teresa.site/resources/theme/img/logo_gmv.svg)  ![Apache Spark](http://spark.apache.org/images/spark-logo.png)\n",
    "\n",
    "# KDD99 Unsupervised Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apache Spark Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "import pyspark\n",
    "sc = pyspark.SparkContext(appName=\"SecurityDataScience\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import * \n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Intrinsic attributes**\n",
    "\n",
    "These attributes are extracted from the headers' area of the network packets.\n",
    "\n",
    "Col|Feature name  | description |\ttype\n",
    "---|--------------|-------------|------------\n",
    "1  |duration \t  |length (number of seconds) of the connection |continuous\n",
    "2  |protocol_type |type of the protocol, e.g. tcp, udp, etc. |discrete\n",
    "3  |service \t  |network service on the destination, e.g., http, telnet, etc. |discrete\n",
    "4  |flag \t      |normal or error status of the connection. The possible status are this: SF, S0, S1, S2, S3, OTH, REJ, RSTO, RSTOS0, SH, RSTRH, SHR \t|discrete \n",
    "5  |src_bytes \t  |number of data bytes from source to destination \t|continuous\n",
    "6  |dst_bytes \t  |number of data bytes from destination to source \t|continuous\n",
    "7  |land \t      |1 if connection is from/to the same host/port; 0 otherwise \t|discrete\n",
    "8  |wrong_fragment|sum of bad checksum packets in a connection \t|continuous\n",
    "9  |urgent \t      |number of urgent packets. Urgent packets are packets with the urgent bit activated \t|continuous\n",
    "\n",
    "\n",
    "**Class attribute**\n",
    "\n",
    "The 42nd attribute is the ***class_attack*** attribute, it indicates which type of connections is each instance: normal or which attack. The values it can take are the following: *anomaly, dict, dict_simple, eject, eject-fail, ffb, ffb_clear, format, format_clear, format-fail, ftp-write, guest, imap, land, load_clear, loadmodule, multihop, perl_clear, perlmagic, phf, rootkit, spy, syslog, teardrop, warez, warezclient, warezmaster, pod, back, ip- sweep, neptune, nmap, portsweep, satan, smurf and normal*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Categories of class attribute **\n",
    "\n",
    "\n",
    "class_attack |Category\n",
    "-------|--------------\n",
    "smurf| dos\n",
    "neptune| dos\n",
    "back| dos\n",
    "teardrop| dos\n",
    "pod| dos\n",
    "land| dos\n",
    "normal|normal\n",
    "satan|probe\n",
    "ipsweep|probe\n",
    "portsweep|probe\n",
    "nmap|probe\n",
    "warezclient|r2l\n",
    "guess_passwd|r2l\n",
    "warezmaster|r2l\n",
    "imap|r2l\n",
    "ftp_write|r2l\n",
    "multihop|r2l\n",
    "phf|r2l\n",
    "spy|r2l\n",
    "buffer_overflow|u2r\n",
    "rootkit|u2r\n",
    "loadmodule|u2r\n",
    "perl|u2r"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "textFileConn = sc.textFile('./data/KDD/KDDTrain+.txt', 4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating the schema\n",
    "\n",
    "#we define the name of the columns\n",
    "\n",
    "columnNames=[\"class_attack\", \"duration\",\"protocol_type\",\"service\",\"flag\",\"src_bytes\",\"dst_bytes\",\"land\",\n",
    "                 \"wrong_fragment\",\"urgent\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#quick fields initialitation all for FloatType\n",
    "connFields = [StructField(colName, FloatType(), True) for colName in columnNames]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we proceed to modify the respective fields so that they reflect the correct data type:\n",
    "connFields[0].dataType = StringType()\n",
    "connFields[2].dataType = StringType()\n",
    "connFields[3].dataType = StringType()\n",
    "connFields[4].dataType = StringType()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can construct our schema, which we will use later below for building the data frame\n",
    "connSchema = StructType(connFields)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Parsing the file\n",
    "def parseReg(p):\n",
    "    return ( p[41]\n",
    "            ,float(p[0])\n",
    "            ,p[1], p[2], p[3] \n",
    "            ,float(p[4])\n",
    "            ,float(p[5])\n",
    "            ,float(p[6])\n",
    "            ,float(p[7])\n",
    "            ,float(p[8])\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "connParsedFile = (textFileConn.map(lambda line: line.split(','))\n",
    "                              .map(parseReg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We are now ready to build our data frame, using the connParsedFile RDD computed above and the schema \n",
    "# variable already calculated:\n",
    "conn = sqlContext.createDataFrame(connParsedFile, connSchema)\n",
    "conn.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn.limit(4).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get all the distint values of class_attack\n",
    "conn.select(\"class_attack\").distinct().toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Encoding categorical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encodeCategorical(df, catName):\n",
    "    #Encode the categorical variable in different columns foreach categories \n",
    "    #and the value is equal to 1 if the category is equal to column name and 0 otherwise. \n",
    "    #Finally drops the categorical variable\n",
    "    \n",
    "    categories = df.select(catName).distinct().toPandas()[catName]\n",
    "    aux = df\n",
    "    for c in categories:\n",
    "        aux = aux.withColumn(c, F.when(df[catName] == c, 1).otherwise(0))\n",
    "        \n",
    "    return aux.drop(catName)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoding *protocol_type*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn.select(\"protocol_type\").distinct().toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "connEncoded = encodeCategorical(conn, \"protocol_type\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "connEncoded.limit(10).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoding *service*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "connEncoded.select(\"service\").distinct().toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "connEncoded = encodeCategorical(connEncoded, \"service\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "connEncoded.limit(10).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoding *flag*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "connEncoded.select(\"flag\").distinct().toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "connEncoded = encodeCategorical(connEncoded, \"flag\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "connEncoded.limit(10).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Encoding *class_attack* (**label**) like Integers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "connEncoded.select(\"class_attack\").distinct().toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = connEncoded.select(\"class_attack\").distinct().toPandas()[\"class_attack\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictCategories = dict((v,int(k)) for (k,v) in categories.to_dict().items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def categoriesToInt(cat):\n",
    "    return dictCategories[cat]\n",
    "\n",
    "udfCategoriesToInt = udf(categoriesToInt, IntegerType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "connEncoded = connEncoded.withColumn(\"class_attack\", udfCategoriesToInt(\"class_attack\") )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "connEncoded.limit(10).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Input Normalization\n",
    "\n",
    "http://spark.apache.org/docs/latest/api/python/pyspark.ml.html#module-pyspark.ml.feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.regression import LabeledPoint\n",
    "from pyspark.ml.feature import StandardScaler\n",
    "from pyspark.ml.linalg import DenseVector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "connEncoded.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = connEncoded.drop(\"class_attack\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "features.limit(10).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats = features.describe().toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "minValue = np.array(stats[stats.summary==\"min\"].values[0][1:], float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxValue = np.array(stats[stats.summary==\"max\"].values[0][1:], float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def minMaxScaler(minV, maxV, row):\n",
    "    return DenseVector([(row[i]-minV[i])/(maxV[i]-minV[i]) for i in range(len(row))])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labeledData = connEncoded.rdd.map(lambda x: (x[0], minMaxScaler(minValue, maxValue, x[1:])))\n",
    "# Apache Spark 1.6.0\n",
    "#labeledData = connEncoded.map(lambda x: (x[0], minMaxScaler(minValue, maxValue, x[1:])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labeledDataFrame = sqlContext.createDataFrame(labeledData, [\"label\", \"features\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labeledDataFrame.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Principal Component Analysis (PCA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v = labeledDataFrame.limit(1).toPandas()[\"features\"].values[0]\n",
    "n_features = len(v.array)\n",
    "print(\"Total number of features: %d\" %n_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimateCovariance(df):\n",
    "    \"\"\"Compute the covariance matrix for a given dataframe.\n",
    "\n",
    "    Note:\n",
    "        The multi-dimensional covariance array should be calculated using outer products.  Don't\n",
    "        forget to normalize the data by first subtracting the mean.\n",
    "\n",
    "    Args:\n",
    "        df:  A Spark dataframe with a column named 'features', which (column) consists of DenseVectors.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: A multi-dimensional array where the number of rows and columns both equal the\n",
    "            length of the arrays in the input dataframe.\n",
    "    \"\"\"\n",
    "    m = df.select(df['features']).rdd.map(lambda x: x[0]).mean()\n",
    "    dfZeroMean = df.select(df['features']).rdd.map(lambda x:   x[0]).map(lambda x: x-m)  # subtract the mean\n",
    "\n",
    "    return dfZeroMean.map(lambda x: np.outer(x,x)).sum()/df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.linalg import eigh\n",
    "\n",
    "def pca(df, k=2):\n",
    "    \"\"\"Computes the top `k` principal components, corresponding scores, and all eigenvalues.\n",
    "\n",
    "    Note:\n",
    "        All eigenvalues should be returned in sorted order (largest to smallest). `eigh` returns\n",
    "        each eigenvectors as a column.  This function should also return eigenvectors as columns.\n",
    "\n",
    "    Args:\n",
    "        df: A Spark dataframe with a 'features' column, which (column) consists of DenseVectors.\n",
    "        k (int): The number of principal components to return.\n",
    "\n",
    "    Returns:\n",
    "        tuple of (np.ndarray, RDD of np.ndarray, np.ndarray): A tuple of (eigenvectors, `RDD` of\n",
    "        scores, eigenvalues).  Eigenvectors is a multi-dimensional array where the number of\n",
    "        rows equals the length of the arrays in the input `RDD` and the number of columns equals\n",
    "        `k`.  The `RDD` of scores has the same number of rows as `data` and consists of arrays\n",
    "        of length `k`.  Eigenvalues is an array of length d (the number of features).\n",
    "     \"\"\"\n",
    "    cov = estimateCovariance(df)\n",
    "    col = cov.shape[1]\n",
    "    eigVals, eigVecs = eigh(cov)\n",
    "    inds = np.argsort(eigVals)\n",
    "    eigVecs = eigVecs.T[inds[-1:-(col+1):-1]]  \n",
    "    components = eigVecs[0:k]\n",
    "    eigVals = eigVals[inds[-1:-(col+1):-1]]  # sort eigenvals\n",
    "    score = df.select(df['features']).rdd.map(lambda x: x[0]).map(lambda x: np.dot(x, components.T) )\n",
    "    # Return the `k` principal components, `k` scores, and all eigenvalues\n",
    "\n",
    "    return components.T, score, eigVals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comp, score, eigVals = pca(labeledDataFrame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "varianceExplained = eigVals.cumsum()/eigVals.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "varianceExplained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(1 - varianceExplained, drawstyle = 'steps-post')\n",
    "plt.title('PCA Reconstruction Error');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_factors = ((1 - varianceExplained) > 0.10).sum()\n",
    "print(\"Number of factors with 10% of reonstraction Error: \", n_factors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2.1 Apache Spark Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Apache Spark API\n",
    "pca = PCA(k = n_factors, inputCol=\"features\", outputCol=\"pca_features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labeledDataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pcaModel = pca.fit(labeledDataFrame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pcaDataFrame = pcaModel.transform(labeledDataFrame).drop(\"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pcaDataFrame.limit(10).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sampling with replacement with the 30% of data\n",
    "trainingData = pcaDataFrame.sample(withReplacement = True, fraction = 0.30).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainingData.limit(10).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Cluster metrics\n",
    "\n",
    "Two desirable objectives for any cluster assignment:\n",
    "* **homogeneity**: each cluster contains only members of a single class.\n",
    "* **completeness**: all members of a given class are assigned to the same cluster.\n",
    "\n",
    "The main cluster metrics are:\n",
    "\n",
    "* **Homogeneity Score**: A clustering result satisfies homogeneity if all of its clusters contain only data points which are members of a single class.\n",
    "    * Bounded scores: 0.0 is as bad as it can be, 1.0 is a perfect score\n",
    "\n",
    "* **Completeness Score**: A clustering result satisfies completeness if all the data points that are members of a given class are elements of the same cluster.\n",
    "    * Bounded scores: 0.0 is as bad as it can be, 1.0 is a perfect score\n",
    "\n",
    "* **V measure Scores** : the harmonic mean between homogeneity and completeness: v = 2 * (homogeneity * completeness) / (homogeneity + completeness)\n",
    "    * Bounded scores: 0.0 is as bad as it can be, 1.0 is a perfect score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 K-means"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* *k:* number of desired clusters.\n",
    "* *maxIter:* maximum number of iterations to run.\n",
    "* *initMode:* specifies either random initialization or initialization via k-means.\n",
    "* *initSteps:* determines the number of steps in the k-means|| algorithm.\n",
    "* *tol:* determines the distance threshold within which we consider k-means to have converged.\n",
    "* initialModel:* an optional set of cluster centers used for initialization. If this parameter is supplied, only one run is performed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.clustering import KMeans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.1 Random centroid initialization (5 clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeans(k = 5, initMode = \"random\", initSteps = 10, \n",
    "                featuresCol = \"pca_features\", maxIter = 10, tol = 1e-3 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = kmeans.fit(trainingData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusterCenters = model.clusterCenters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(clusterCenters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictedData = model.transform(pcaDataFrame)\n",
    "predictedData.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictedData.limit(10).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate clustering by computing Within Set Sum of Squared Errors\n",
    "def error(row, centers):\n",
    "    center = centers[row[\"prediction\"]]\n",
    "    point = row[\"pca_features\"]\n",
    "    return np.sqrt(np.sum([x**2 for x in (point - center)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"KMeans error:\" , predictedData.rdd.map(lambda row: error(row, clusterCenters)).reduce(lambda a,b : a+b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeansMeasures = predictedData.groupBy(predictedData.label, predictedData.prediction).count().toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#completeness (all members of a given class are assigned to the same cluster)\n",
    "print(\"Completeness:\" , 1.0*np.sum(kmeansMeasures.groupby(\"label\")[\"count\"].max())/np.sum(kmeansMeasures[\"count\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeansMeasures[kmeansMeasures.label==3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#homogeneity (each cluster contains only members of a single class.)\n",
    "print(\"Homogeneity:\", 1.0*np.sum(kmeansMeasures.groupby(\"prediction\")[\"count\"].max())/np.sum(kmeansMeasures[\"count\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "kmeansMeasures[kmeansMeasures.prediction==2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.2 Finding the optimal number of clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kmeansClusterMeasures(n_Clusters, dataFrame):\n",
    "    #build a kmean cluster from dataFrame and with n_Clusters centroides\n",
    "    #and returns the error, completeness and homogeneity of kmean's model\n",
    "\n",
    "    trainingData, testData = dataFrame.randomSplit([70.0, 30.0])\n",
    "    \n",
    "    kmeans = KMeans(k = n_Clusters, initMode = \"random\", initSteps = 10, \n",
    "                    featuresCol = \"pca_features\", maxIter = 10, tol = 1e-3 )\n",
    "    model = kmeans.fit(trainingData)\n",
    "    centers = model.clusterCenters()\n",
    "    \n",
    "    predictedData = model.transform(testData)\n",
    "    predictedData.cache()\n",
    "    \n",
    "    errorKmeans =  predictedData.rdd.map(lambda row: error(row, centers)).reduce(lambda a,b : a+b)\n",
    "    \n",
    "    kmeansMeasures = predictedData.groupBy(predictedData.label, predictedData.prediction).count().toPandas()\n",
    "    completeness = 1.0*np.sum(kmeansMeasures.groupby(\"label\")[\"count\"].max())/np.sum(kmeansMeasures[\"count\"])\n",
    "    homogeneity = 1.0*np.sum(kmeansMeasures.groupby(\"prediction\")[\"count\"].max())/np.sum(kmeansMeasures[\"count\"])\n",
    "    \n",
    "    \n",
    "    return n_Clusters, errorKmeans, completeness, homogeneity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def findBestCluster(minNClusters, maxNClusters, dataFrame):\n",
    "    nClusters = np.arange(minNClusters, maxNClusters+1)\n",
    "    kmeansMeasures = [kmeansClusterMeasures(n, trainingData) for n in nClusters]\n",
    "    return pd.DataFrame(kmeansMeasures, \n",
    "                        columns = [\"nClusters\", \"error\", \"completeness\", \"homogeneity\"],\n",
    "                       index = nClusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "print(kmeansClusterMeasures(5, trainingData))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "kmeansMeasures = findBestCluster(4, 20, trainingData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "kmeansMeasures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 3)\n",
    "kmeansMeasures.error.plot(ax=axes[0],title=\"Kmeans Error\", grid = True, figsize=(15,5))\n",
    "axes[0].set_xlabel(\"Number of Clusters\")\n",
    "kmeansMeasures.completeness.plot(ax=axes[1],title=\"Completeness\", grid = True)\n",
    "axes[1].set_xlabel(\"Number of Clusters\")\n",
    "kmeansMeasures.homogeneity.plot(ax=axes[2],title=\"Homogeneity\", grid = True)\n",
    "axes[2].set_xlabel(\"Number of Clusters\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create the clusters with  11 centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "kmeans = KMeans(k = 11, initMode = \"random\", initSteps = 10, \n",
    "                featuresCol = \"pca_features\", maxIter = 10, tol = 1e-3 )\n",
    "model = kmeans.fit(trainingData)\n",
    "clusterCenters = model.clusterCenters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "predictedData = model.transform(pcaDataFrame)\n",
    "predictedData.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictedData.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualize the results on PCA-reduced data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import PCA\n",
    "from pyspark.sql import Row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(k=2, inputCol=\"pca_features\", outputCol=\"pca_2D\")\n",
    "modelPCA = pca.fit(predictedData)\n",
    "projectedDT = modelPCA.transform(predictedData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "projectedDT.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusterCentersDF = sc.parallelize(\n",
    "    [Row(n, DenseVector(cluster.tolist())) for n, cluster in zip(range(len(clusterCenters)), clusterCenters)]\n",
    ").toDF([\"prediction\", \"pca_features\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusterCentersDF.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusterCenters2D = modelPCA.transform(clusterCentersDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "centers2D = clusterCenters2D.rdd.map(lambda row: (row[\"prediction\"], row[\"pca_2D\"]))\\\n",
    "                            .map(lambda v: Row(prediction = v[0], x1 = v[1].values.tolist()[0],\\\n",
    "                                                     x2 = v[1].values.tolist()[1]))\\\n",
    "                            .toDF().toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_points = 10000\n",
    "points2D = projectedDT.limit(n_points).rdd\\\n",
    "                      .map(lambda row: (row[\"label\"], row[\"prediction\"], row[\"pca_2D\"]))\\\n",
    "                      .map(lambda v: Row(label = v[0], \n",
    "                                                prediction = v[1], \n",
    "                                                x1 = v[2].values.tolist()[0], \n",
    "                                                x2 = v[2].values.tolist()[1]))\\\n",
    "                       .toDF().toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "points2D.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set colors\n",
    "\n",
    "from matplotlib import colors\n",
    "import six\n",
    "colors_ = list(six.iteritems(colors.cnames))\n",
    "\n",
    "# Add the single letter colors.\n",
    "for name, rgb in six.iteritems(colors.ColorConverter.colors):\n",
    "    hex_ = colors.rgb2hex(rgb)\n",
    "    colors_.append((name, hex_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(ncols=2, figsize=(14, 7) )\n",
    "ax1, ax2 = axes.ravel()\n",
    "\n",
    "# Plot the clusters\n",
    "for i, color in zip(set(points2D.prediction), colors_):\n",
    "    idx = np.where(points2D.prediction == i)\n",
    "    ax1.scatter(points2D.values[idx, 2], points2D.values[idx, 3], c=color, s = 10, label=i, alpha = 0.25, edgecolors='none')\n",
    "    # Plot the centroids as X\n",
    "    ax1.scatter(centers2D.values[i, 1], centers2D.values[i, 2],\n",
    "            marker='x', s=169, linewidths=3,\n",
    "            color=color, zorder=10)\n",
    "ax1.set_title('K-mean prediction - Random centroid initialization (11 clusters)')\n",
    "ax1.legend()\n",
    "\n",
    "\n",
    "\n",
    "#Plot the category values\n",
    "for i, color in zip(set(points2D.label), colors_):\n",
    "    idx = np.where(points2D.label == i)\n",
    "    ax2.scatter(points2D.values[idx, 2], points2D.values[idx, 3], c=color, s = 10, label=i, alpha = 0.25, edgecolors='none')\n",
    "    \n",
    "ax2.set_title('Category Attacks (11 categories)')\n",
    "ax2.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualize the results on PCA-reduced data 3D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(k=3, inputCol=\"pca_features\", outputCol=\"pca_3D\")\n",
    "modelPCA = pca.fit(predictedData)\n",
    "projectedDT = modelPCA.transform(predictedData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "projectedDT.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusterCentersDF = sc.parallelize(\n",
    "    [Row(n, DenseVector(cluster.tolist())) for n, cluster in zip(range(len(clusterCenters)), clusterCenters)]\n",
    ").toDF([\"prediction\", \"pca_features\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusterCentersDF.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusterCenters3D = modelPCA.transform(clusterCentersDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "centers3D = clusterCenters3D.rdd.map(lambda row: (row[\"prediction\"], row[\"pca_3D\"]))\\\n",
    "                            .map(lambda v: Row(prediction = v[0], \n",
    "                                                       x1 = v[1].values.tolist()[0], \n",
    "                                                       x2 = v[1].values.tolist()[1],\n",
    "                                                       x3 = v[1].values.tolist()[2]))\\\n",
    "                            .toDF().toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "centers3D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_points = 1000\n",
    "points3D = projectedDT.limit(n_points).rdd\\\n",
    "                      .map(lambda row: (row[\"label\"], row[\"prediction\"], row[\"pca_3D\"]))\\\n",
    "                      .map(lambda v: Row(label = v[0], \n",
    "                                                prediction = v[1], \n",
    "                                                x1 = v[2].values.tolist()[0], \n",
    "                                                x2 = v[2].values.tolist()[1],\n",
    "                                                x3 = v[2].values.tolist()[2]))\\\n",
    "                       .toDF().toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "fig = plt.figure(figsize=(8,8))\n",
    "ax = fig.gca(projection='3d')\n",
    "\n",
    "# Plot the clusters\n",
    "for i, color in zip(set(points3D.prediction), colors_):\n",
    "    idx = np.where( points3D.prediction == i)\n",
    "    ax.scatter(points3D.values[idx, 2], points3D.values[idx, 3], points3D.values[idx, 4],\n",
    "               c=color, label=i, s=50, alpha = 0.3, \n",
    "               edgecolors='none')\n",
    "    # Plot the centroids as X\n",
    "    ax.scatter(centers3D.values[i, 1], centers3D.values[i, 2], centers3D.values[i, 3], \n",
    "               marker='x', linewidths=3, s = 150,\n",
    "               color=color, zorder=10)\n",
    "ax.set_title('K-mean prediction - Random centroid initialization (11 clusters)')\n",
    "ax.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
